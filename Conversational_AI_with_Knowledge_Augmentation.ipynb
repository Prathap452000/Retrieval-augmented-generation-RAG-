{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pinecone-client datasets faiss flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, request, jsonify\n",
    "from retriever import retrieve_context\n",
    "from generator import generate_response\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    user_input = request.json.get(\"message\", \"\")\n",
    "    retrieved_context = retrieve_context(user_input)\n",
    "    response = generate_response(user_input, retrieved_context)\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "\n",
    "PINECONE_API_KEY = \"pnc-1234abcd5678xyz\"\n",
    "PINECONE_ENV = \"us-west1-gcp\"\n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "index_name = \"chat-memory\"\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=768)\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "client = OpenAI(api_key=\"sk-1234abcd5678xyz\")\n",
    "\n",
    "def embed_text(text):\n",
    "    response = client.embeddings.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    return response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def store_data():\n",
    "    dataset = load_dataset(\"ag_news\", split=\"train[:100]\")\n",
    "    for data in dataset:\n",
    "        text = data[\"text\"]\n",
    "        vector = embed_text(text)\n",
    "        index.upsert([(text, vector)])\n",
    "\n",
    "store_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import numpy as np\n",
    "from vector_store import index, embed_text\n",
    "\n",
    "def retrieve_context(query):\n",
    "    query_vector = embed_text(query)\n",
    "    results = index.query(vector=query_vector, top_k=5, include_metadata=True)\n",
    "    return \" \".join([match[\"id\"] for match in results[\"matches\"]]) if results.get(\"matches\") else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"sk-1234abcd5678xyz\")\n",
    "\n",
    "def generate_response(user_input, context):\n",
    "    prompt = f\"User: {user_input}\\nContext: {context}\\nAssistant:\"\n",
    "    response = client.chat.completions.create(model=\"gpt-4\", messages=[{\"role\": \"system\", \"content\": prompt}])\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST http://localhost:5000/chat\n",
    "{\n",
    "    \"message\": \"Tell me about AI ethics.\"\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
